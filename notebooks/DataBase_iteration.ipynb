{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6342d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from catboost import CatBoostRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d3c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Proyecto_ML.csv\")\n",
    "\n",
    "# Define targets\n",
    "targets = [\"Copies Sold\", \"Wishlists\", \"bayesian_score\"]\n",
    "\n",
    "# Define feature set (exclude target variables)\n",
    "drop_cols = targets + [\"appid\", \"name\", \"release_date\", \"developers\", \"publishers\"]\n",
    "X = df.drop(columns=drop_cols, errors='ignore')\n",
    "X = X.select_dtypes(include='number')  # Keep only numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c575ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "for i, target in enumerate(targets[:2]):  # Only plot Copies Sold and Wishlists (skip bayesian_score)\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    \n",
    "    # Use log scale to visualize the extreme skew\n",
    "    sns.histplot(df[target], bins=50, kde=True)\n",
    "    plt.title(f'Original Distribution: {target}')\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('target_distribution_before.png', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7adfeaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log transformation functions\n",
    "def log_transform(y):\n",
    "    return np.log1p(y)  # log1p = log(1+y) to handle zeros\n",
    "\n",
    "def inverse_log_transform(y):\n",
    "    return np.expm1(y)  # expm1 = exp(y)-1, inverse of log1p\n",
    "\n",
    "# Define BoxCox transformations - alternative approach\n",
    "from scipy import stats\n",
    "\n",
    "def boxcox_transform(y):\n",
    "    # Add small constant to ensure all values are positive\n",
    "    y_positive = y + 1e-10  \n",
    "    y_transformed, lambda_value = stats.boxcox(y_positive)\n",
    "    # Store lambda for inverse transform\n",
    "    boxcox_transform.lambda_value = lambda_value\n",
    "    return y_transformed\n",
    "\n",
    "def inverse_boxcox_transform(y):\n",
    "    # Use stored lambda for inverse transform\n",
    "    lambda_value = boxcox_transform.lambda_value\n",
    "    if lambda_value == 0:\n",
    "        return np.exp(y) - 1e-10\n",
    "    else:\n",
    "        return (lambda_value * y + 1) ** (1/lambda_value) - 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "978dcaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformed_model(X, y, target_name, transform='log', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Train a CatBoost model with transformed target.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : DataFrame\n",
    "        Feature matrix\n",
    "    y : Series\n",
    "        Target variable\n",
    "    target_name : str\n",
    "        Name of the target variable\n",
    "    transform : str, default='log'\n",
    "        Transformation to apply ('log' or 'boxcox')\n",
    "    test_size : float, default=0.2\n",
    "        Proportion of the dataset to include in the test split\n",
    "    random_state : int, default=42\n",
    "        Controls the shuffling in train/test split\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : TransformedTargetRegressor\n",
    "        Trained model with target transformation\n",
    "    X_test : DataFrame\n",
    "        Test features\n",
    "    y_test : Series\n",
    "        Test target values\n",
    "    metrics : dict\n",
    "        Performance metrics\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Choose transformation\n",
    "    if transform == 'log':\n",
    "        transformer = FunctionTransformer(log_transform, inverse_func=inverse_log_transform)\n",
    "    elif transform == 'boxcox':\n",
    "        transformer = FunctionTransformer(boxcox_transform, inverse_func=inverse_boxcox_transform)\n",
    "    else:\n",
    "        raise ValueError(\"Transform must be 'log' or 'boxcox'\")\n",
    "    \n",
    "    # Create CatBoost regressor\n",
    "    regressor = CatBoostRegressor(\n",
    "        iterations=500,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        loss_function='RMSE',\n",
    "        random_seed=random_state,\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    # Create TransformedTargetRegressor\n",
    "    model = TransformedTargetRegressor(\n",
    "        regressor=regressor,\n",
    "        transformer=transformer\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions (automatically un-transforms the target)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    metrics = {\n",
    "        'r2': r2,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nResults for {target_name} with {transform} transformation:\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    \n",
    "    return model, X_test, y_test, y_pred, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81738ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.8026347\ttotal: 154ms\tremaining: 1m 16s\n",
      "100:\tlearn: 0.5951611\ttotal: 612ms\tremaining: 2.42s\n",
      "200:\tlearn: 0.5282901\ttotal: 1.06s\tremaining: 1.57s\n",
      "300:\tlearn: 0.4922613\ttotal: 1.52s\tremaining: 1.01s\n",
      "400:\tlearn: 0.4666989\ttotal: 1.95s\tremaining: 481ms\n",
      "499:\tlearn: 0.4458664\ttotal: 2.37s\tremaining: 0us\n",
      "\n",
      "Results for Copies Sold with log transformation:\n",
      "R² Score: 0.7720\n",
      "MAE: 160349.26\n",
      "RMSE: 914071.62\n",
      "Model saved as transformed_log_Copies_Sold.pkl\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target == \u001b[33m\"\u001b[39m\u001b[33mbayesian_score\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m transform == \u001b[33m\"\u001b[39m\u001b[33mboxcox\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m model, X_test, y_test, y_pred, metrics = \u001b[43mtrain_transformed_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m key = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransform\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     24\u001b[39m results[key] = {\n\u001b[32m     25\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m: model,\n\u001b[32m     26\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mX_test\u001b[39m\u001b[33m'\u001b[39m: X_test,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m'\u001b[39m: metrics\n\u001b[32m     30\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mtrain_transformed_model\u001b[39m\u001b[34m(X, y, target_name, transform, test_size, random_state)\u001b[39m\n\u001b[32m     55\u001b[39m model = TransformedTargetRegressor(\n\u001b[32m     56\u001b[39m     regressor=regressor,\n\u001b[32m     57\u001b[39m     transformer=transformer\n\u001b[32m     58\u001b[39m )\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Make predictions (automatically un-transforms the target)\u001b[39;00m\n\u001b[32m     64\u001b[39m y_pred = model.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\compose\\_target.py:277\u001b[39m, in \u001b[36mTransformedTargetRegressor.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    276\u001b[39m     y_2d = y\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# transform y and convert back to 1d array if needed\u001b[39;00m\n\u001b[32m    280\u001b[39m y_trans = \u001b[38;5;28mself\u001b[39m.transformer_.transform(y_2d)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\compose\\_target.py:204\u001b[39m, in \u001b[36mTransformedTargetRegressor._fit_transformer\u001b[39m\u001b[34m(self, y)\u001b[39m\n\u001b[32m    199\u001b[39m     \u001b[38;5;28mself\u001b[39m.transformer_.set_output(transform=\u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# XXX: sample_weight is not currently passed to the\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# transformer. However, if transformer starts using sample_weight, the\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# code should be modified accordingly. At the time to consider the\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;66;03m# sample_prop feature, it is also a good use case to be considered.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.check_inverse:\n\u001b[32m    206\u001b[39m     idx_selected = \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, y.shape[\u001b[32m0\u001b[39m] // \u001b[32m10\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:242\u001b[39m, in \u001b[36mFunctionTransformer.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    240\u001b[39m X = \u001b[38;5;28mself\u001b[39m._check_input(X, reset=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.check_inverse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.inverse_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_inverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:195\u001b[39m, in \u001b[36mFunctionTransformer._check_inverse_transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check that func and inverse_func are the inverse.\"\"\"\u001b[39;00m\n\u001b[32m    194\u001b[39m idx_selected = \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, X.shape[\u001b[32m0\u001b[39m] // \u001b[32m100\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m X_round_trip = \u001b[38;5;28mself\u001b[39m.inverse_transform(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_selected\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    198\u001b[39m     dtypes = [X.dtype]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:260\u001b[39m, in \u001b[36mFunctionTransformer.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform X using the forward function.\u001b[39;00m\n\u001b[32m    247\u001b[39m \n\u001b[32m    248\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    257\u001b[39m \u001b[33;03m    Transformed input.\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    259\u001b[39m X = \u001b[38;5;28mself\u001b[39m._check_input(X, reset=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkw_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m output_config = _get_output_config(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)[\u001b[33m\"\u001b[39m\u001b[33mdense\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(out, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.feature_names_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    264\u001b[39m     \u001b[38;5;66;03m# check the consistency between the column provided by `transform` and\u001b[39;00m\n\u001b[32m    265\u001b[39m     \u001b[38;5;66;03m# the the column names provided by `get_feature_names_out`.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:387\u001b[39m, in \u001b[36mFunctionTransformer._transform\u001b[39m\u001b[34m(self, X, func, kw_args)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    385\u001b[39m     func = _identity\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkw_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkw_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mboxcox_transform\u001b[39m\u001b[34m(y)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mboxcox_transform\u001b[39m(y):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Add small constant to ensure all values are positive\u001b[39;00m\n\u001b[32m     13\u001b[39m     y_positive = y + \u001b[32m1e-10\u001b[39m  \n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     y_transformed, lambda_value = \u001b[43mstats\u001b[49m\u001b[43m.\u001b[49m\u001b[43mboxcox\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_positive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Store lambda for inverse transform\u001b[39;00m\n\u001b[32m     16\u001b[39m     boxcox_transform.lambda_value = lambda_value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\artur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_morestats.py:1141\u001b[39m, in \u001b[36mboxcox\u001b[39m\u001b[34m(x, lmbda, alpha, optimizer)\u001b[39m\n\u001b[32m   1138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m special.boxcox(x, lmbda)\n\u001b[32m   1140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.ndim != \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1141\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mData must be 1-dimensional.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.size == \u001b[32m0\u001b[39m:\n\u001b[32m   1144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[31mValueError\u001b[39m: Data must be 1-dimensional."
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "# Train models for each target\n",
    "for target in targets:\n",
    "    y = df[target]\n",
    "    \n",
    "    # Skip targets with non-positive values for BoxCox transformation\n",
    "    if (y <= 0).any() and target != \"bayesian_score\":\n",
    "        print(f\"Warning: {target} contains non-positive values. Using log transform only.\")\n",
    "        transforms = ['log']\n",
    "    else:\n",
    "        transforms = ['log', 'boxcox']\n",
    "    \n",
    "    for transform in transforms:\n",
    "        # Skip BoxCox for bayesian_score (tends to be more normal already)\n",
    "        if target == \"bayesian_score\" and transform == \"boxcox\":\n",
    "            continue\n",
    "            \n",
    "        model, X_test, y_test, y_pred, metrics = train_transformed_model(\n",
    "            X, y, target, transform=transform\n",
    "        )\n",
    "        \n",
    "        key = f\"{target}_{transform}\"\n",
    "        results[key] = {\n",
    "            'model': model,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        # Save the model\n",
    "        import joblib\n",
    "        model_filename = f\"transformed_{transform}_{target.replace(' ', '_')}.pkl\"\n",
    "        joblib.dump(model, model_filename)\n",
    "        print(f\"Model saved as {model_filename}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9a164ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training log-transformed model for Copies Sold...\n",
      "0:\tlearn: 1.8026347\ttotal: 3.78ms\tremaining: 1.89s\n",
      "100:\tlearn: 0.5951611\ttotal: 537ms\tremaining: 2.12s\n",
      "200:\tlearn: 0.5282901\ttotal: 1.03s\tremaining: 1.53s\n",
      "300:\tlearn: 0.4922613\ttotal: 1.54s\tremaining: 1.02s\n",
      "400:\tlearn: 0.4666989\ttotal: 2.04s\tremaining: 503ms\n",
      "499:\tlearn: 0.4458664\ttotal: 2.53s\tremaining: 0us\n",
      "\n",
      "Results for Copies Sold with log transformation:\n",
      "R² Score: 0.7720\n",
      "MAE: 160349.26\n",
      "RMSE: 914071.62\n",
      "Model saved as log_transformed_Copies_Sold.pkl\n",
      "\n",
      "Training log-transformed model for Wishlists...\n",
      "0:\tlearn: 1.6173342\ttotal: 4.21ms\tremaining: 2.1s\n",
      "100:\tlearn: 0.5128806\ttotal: 484ms\tremaining: 1.91s\n",
      "200:\tlearn: 0.4744640\ttotal: 981ms\tremaining: 1.46s\n",
      "300:\tlearn: 0.4478259\ttotal: 1.44s\tremaining: 953ms\n",
      "400:\tlearn: 0.4273571\ttotal: 1.92s\tremaining: 473ms\n",
      "499:\tlearn: 0.4124890\ttotal: 2.38s\tremaining: 0us\n",
      "\n",
      "Results for Wishlists with log transformation:\n",
      "R² Score: 0.7867\n",
      "MAE: 29436.44\n",
      "RMSE: 118690.82\n",
      "Model saved as log_transformed_Wishlists.pkl\n",
      "\n",
      "Training log-transformed model for bayesian_score...\n",
      "0:\tlearn: 0.1552978\ttotal: 3.77ms\tremaining: 1.88s\n",
      "100:\tlearn: 0.1334168\ttotal: 505ms\tremaining: 1.99s\n",
      "200:\tlearn: 0.1249370\ttotal: 1.01s\tremaining: 1.5s\n",
      "300:\tlearn: 0.1177860\ttotal: 1.48s\tremaining: 981ms\n",
      "400:\tlearn: 0.1116687\ttotal: 1.96s\tremaining: 484ms\n",
      "499:\tlearn: 0.1066948\ttotal: 2.46s\tremaining: 0us\n",
      "\n",
      "Results for bayesian_score with log transformation:\n",
      "R² Score: 0.3487\n",
      "MAE: 6.98\n",
      "RMSE: 9.14\n",
      "Model saved as log_transformed_bayesian_score.pkl\n",
      "Could not compare with original model for Copies Sold: [Errno 2] No such file or directory: 'catboost_model_Copies_Sold.pkl'\n",
      "\n",
      "Improvement Summary:\n",
      "        Target  Original R²  Log-Transformed R²  Improvement  Improvement %\n",
      "     Wishlists       0.7525              0.7867       0.0343         4.5563\n",
      "bayesian_score       0.3865              0.3487      -0.0379        -9.7968\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from catboost import CatBoostRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"Proyecto_ML.csv\")\n",
    "\n",
    "# Define targets\n",
    "targets = [\"Copies Sold\", \"Wishlists\", \"bayesian_score\"]\n",
    "\n",
    "# Define feature set (exclude target variables)\n",
    "drop_cols = targets + [\"appid\", \"name\", \"release_date\", \"developers\", \"publishers\"]\n",
    "X = df.drop(columns=drop_cols, errors='ignore')\n",
    "X = X.select_dtypes(include='number')  # Keep only numeric features\n",
    "\n",
    "# Plot original target distribution\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "for i, target in enumerate(targets[:2]):  # Only plot Copies Sold and Wishlists (skip bayesian_score)\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    \n",
    "    # Use log scale to visualize the extreme skew\n",
    "    sns.histplot(df[target], bins=50, kde=True)\n",
    "    plt.title(f'Original Distribution: {target}')\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('target_distribution_before.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Define log transformation functions\n",
    "def log_transform(y):\n",
    "    return np.log1p(y)  # log1p = log(1+y) to handle zeros\n",
    "\n",
    "def inverse_log_transform(y):\n",
    "    return np.expm1(y)  # expm1 = exp(y)-1, inverse of log1p\n",
    "\n",
    "# Create a function to train models with log-transformed targets\n",
    "def train_log_model(X, y, target_name, test_size=0.2, random_state=42):\n",
    "    \"\"\"Train a CatBoost model with log-transformed target.\"\"\"\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Create log transformer\n",
    "    transformer = FunctionTransformer(log_transform, inverse_func=inverse_log_transform)\n",
    "    \n",
    "    # Create CatBoost regressor\n",
    "    regressor = CatBoostRegressor(\n",
    "        iterations=500,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        loss_function='RMSE',\n",
    "        random_seed=random_state,\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    # Create TransformedTargetRegressor\n",
    "    model = TransformedTargetRegressor(\n",
    "        regressor=regressor,\n",
    "        transformer=transformer\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions (automatically un-transforms the target)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    metrics = {\n",
    "        'r2': r2,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nResults for {target_name} with log transformation:\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    \n",
    "    return model, X_test, y_test, y_pred, metrics\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Train log-transformed models for each target\n",
    "for target in targets:\n",
    "    y = df[target]\n",
    "    \n",
    "    print(f\"\\nTraining log-transformed model for {target}...\")\n",
    "    model, X_test, y_test, y_pred, metrics = train_log_model(X, y, target)\n",
    "    \n",
    "    key = f\"{target}_log\"\n",
    "    results[key] = {\n",
    "        'model': model,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    \n",
    "    # Save the model\n",
    "    model_filename = f\"log_transformed_{target.replace(' ', '_')}.pkl\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i, target in enumerate(targets):\n",
    "    key = f\"{target}_log\"\n",
    "    \n",
    "    plt.subplot(len(targets), 1, i+1)\n",
    "    \n",
    "    y_test = results[key]['y_test']\n",
    "    y_pred = results[key]['y_pred']\n",
    "    r2 = results[key]['metrics']['r2']\n",
    "    \n",
    "    # Plot original scale\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5, edgecolor='white', s=40)\n",
    "    \n",
    "    # Add reference line\n",
    "    max_val = max(y_test.max(), y_pred.max())\n",
    "    min_val = min(y_test.min(), y_pred.min())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)\n",
    "    \n",
    "    # Use log scale for Copies Sold and Wishlists\n",
    "    if target in [\"Copies Sold\", \"Wishlists\"]:\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "    \n",
    "    plt.title(f'{target} - Log Transform (R² = {r2:.4f})')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('log_transformed_predictions.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Try to compare with original models if available\n",
    "improvement_summary = []\n",
    "\n",
    "for target in targets:\n",
    "    try:\n",
    "        # Load original model (you may need to adjust the filename)\n",
    "        original_model_path = f\"catboost_model_{target.replace(' ', '_')}.pkl\"\n",
    "        original_model = joblib.load(original_model_path)\n",
    "        \n",
    "        # Make predictions with original model\n",
    "        original_preds = original_model.predict(results[f\"{target}_log\"]['X_test'])\n",
    "        original_r2 = r2_score(results[f\"{target}_log\"]['y_test'], original_preds)\n",
    "        \n",
    "        # Get transformed model results\n",
    "        transformed_r2 = results[f\"{target}_log\"]['metrics']['r2']\n",
    "        \n",
    "        # Calculate improvement\n",
    "        improvement = transformed_r2 - original_r2\n",
    "        improvement_pct = (improvement / original_r2) * 100 if original_r2 > 0 else np.nan\n",
    "        \n",
    "        improvement_summary.append({\n",
    "            'Target': target,\n",
    "            'Original R²': original_r2,\n",
    "            'Log-Transformed R²': transformed_r2,\n",
    "            'Improvement': improvement,\n",
    "            'Improvement %': improvement_pct\n",
    "        })\n",
    "        \n",
    "        # Plot comparison of residuals\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Original model residuals\n",
    "        plt.subplot(1, 2, 1)\n",
    "        residuals_orig = original_preds - results[f\"{target}_log\"]['y_test']\n",
    "        plt.scatter(original_preds, residuals_orig, alpha=0.5)\n",
    "        plt.axhline(y=0, color='r', linestyle='-')\n",
    "        plt.title(f'Original Model Residuals - {target}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Residuals')\n",
    "        \n",
    "        # Transformed model residuals\n",
    "        plt.subplot(1, 2, 2)\n",
    "        residuals_trans = results[f\"{target}_log\"]['y_pred'] - results[f\"{target}_log\"]['y_test']\n",
    "        plt.scatter(results[f\"{target}_log\"]['y_pred'], residuals_trans, alpha=0.5)\n",
    "        plt.axhline(y=0, color='r', linestyle='-')\n",
    "        plt.title(f'Log-Transformed Model Residuals - {target}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Residuals')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'residual_comparison_{target}.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "    except (FileNotFoundError, KeyError) as e:\n",
    "        print(f\"Could not compare with original model for {target}: {e}\")\n",
    "\n",
    "# Display improvement summary\n",
    "if improvement_summary:\n",
    "    improvement_df = pd.DataFrame(improvement_summary)\n",
    "    print(\"\\nImprovement Summary:\")\n",
    "    print(improvement_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "    \n",
    "    # Save summary to CSV\n",
    "    improvement_df.to_csv('log_transformation_improvements.csv', index=False)\n",
    "else:\n",
    "    print(\"\\nNo comparison with original models was possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c2db3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
